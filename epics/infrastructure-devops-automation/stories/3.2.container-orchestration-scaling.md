# Story 3.2: Container Orchestration and Scaling

## Status
Draft

## Story

**As a** platform operator,
**I want** Kubernetes-based container orchestration with auto-scaling and high availability,
**so that** the application can handle varying loads and maintain consistent performance.

## Acceptance Criteria

1. **Kubernetes Cluster Setup**: Production-ready Kubernetes cluster with high availability
2. **Auto-scaling Configuration**: Horizontal and vertical pod autoscaling based on metrics
3. **Load Balancing**: Intelligent load balancing with health checks and traffic distribution
4. **Service Mesh Integration**: Istio service mesh for traffic management and security
5. **High Availability**: Multi-zone deployment with automatic failover capabilities
6. **Resource Management**: Optimized resource allocation and namespace isolation
7. **Rolling Updates**: Zero-downtime deployments with automated rollback
8. **Monitoring Integration**: Comprehensive monitoring of cluster and application metrics

## Tasks / Subtasks

- [ ] Task 1: Setup Production Kubernetes Cluster (AC: 1)
  - [ ] Create multi-node Kubernetes cluster with kubeadm
  - [ ] Configure cluster networking with Calico CNI
  - [ ] Setup etcd backup and recovery procedures
  - [ ] Implement cluster authentication and RBAC
  - [ ] Configure cluster monitoring and logging

- [ ] Task 2: Implement Auto-scaling Mechanisms (AC: 2)
  - [ ] Configure Horizontal Pod Autoscaler (HPA) with custom metrics
  - [ ] Implement Vertical Pod Autoscaler (VPA) for resource optimization
  - [ ] Setup Cluster Autoscaler for node scaling
  - [ ] Create custom metrics for application-specific scaling
  - [ ] Implement predictive scaling based on historical data

- [ ] Task 3: Configure Load Balancing and Ingress (AC: 3)
  - [ ] Setup NGINX Ingress Controller with SSL termination
  - [ ] Configure load balancing algorithms and session affinity
  - [ ] Implement health check probes for all services
  - [ ] Setup external DNS integration for automatic DNS management
  - [ ] Configure rate limiting and traffic shaping

- [ ] Task 4: Integrate Service Mesh (AC: 4)
  - [ ] Install and configure Istio service mesh
  - [ ] Implement mutual TLS between services
  - [ ] Configure traffic routing and canary deployments
  - [ ] Setup service mesh observability with Jaeger and Kiali
  - [ ] Implement circuit breakers and retry policies

- [ ] Task 5: Ensure High Availability (AC: 5)
  - [ ] Configure multi-zone deployment strategies
  - [ ] Implement pod disruption budgets
  - [ ] Setup automatic failover mechanisms
  - [ ] Configure persistent volume replication
  - [ ] Implement disaster recovery procedures

- [ ] Task 6: Optimize Resource Management (AC: 6)
  - [ ] Create namespace-based resource quotas
  - [ ] Implement resource requests and limits
  - [ ] Configure quality of service classes
  - [ ] Setup resource monitoring and optimization recommendations
  - [ ] Implement cost allocation and chargeback

- [ ] Task 7: Implement Rolling Updates and Rollback (AC: 7)
  - [ ] Configure deployment strategies (rolling update, blue-green)
  - [ ] Implement automated health checks during deployments
  - [ ] Setup automatic rollback on deployment failures
  - [ ] Create deployment validation and smoke tests
  - [ ] Implement canary deployment capabilities

- [ ] Task 8: Setup Comprehensive Monitoring (AC: 8)
  - [ ] Deploy Prometheus for metrics collection
  - [ ] Configure Grafana dashboards for cluster monitoring
  - [ ] Implement application performance monitoring (APM)
  - [ ] Setup alerting with AlertManager
  - [ ] Create SLI/SLO monitoring and reporting

## Dev Notes

### Relevant Source Tree Information

**Kubernetes Configuration Structure:**
```
k8s/
├── cluster/
│   ├── cluster-setup.yaml        # NEW FILE - Cluster configuration
│   ├── networking.yaml           # NEW FILE - Network policies
│   ├── rbac.yaml                 # NEW FILE - RBAC configuration
│   ├── storage-classes.yaml      # NEW FILE - Storage configuration
│   └── monitoring.yaml           # NEW FILE - Cluster monitoring
├── namespaces/
│   ├── production.yaml           # NEW FILE - Production namespace
│   ├── staging.yaml              # NEW FILE - Staging namespace
│   ├── development.yaml          # NEW FILE - Development namespace
│   └── monitoring.yaml           # NEW FILE - Monitoring namespace
├── applications/
│   ├── teams-bot/
│   │   ├── deployment.yaml       # NEW FILE - Bot deployment
│   │   ├── service.yaml          # NEW FILE - Bot service
│   │   ├── hpa.yaml              # NEW FILE - Horizontal pod autoscaler
│   │   └── vpa.yaml              # NEW FILE - Vertical pod autoscaler
│   ├── openwebui/
│   │   ├── deployment.yaml       # NEW FILE - OpenWebUI deployment
│   │   ├── service.yaml          # NEW FILE - OpenWebUI service
│   │   └── ingress.yaml          # NEW FILE - OpenWebUI ingress
│   ├── planner-mcp-server/
│   │   ├── deployment.yaml       # NEW FILE - MCP server deployment
│   │   ├── service.yaml          # NEW FILE - MCP server service
│   │   └── configmap.yaml        # NEW FILE - Configuration
│   └── mcpo-proxy/
│       ├── deployment.yaml       # NEW FILE - Proxy deployment
│       ├── service.yaml          # NEW FILE - Proxy service
│       └── networkpolicy.yaml    # NEW FILE - Network policies
├── infrastructure/
│   ├── postgresql/
│   │   ├── statefulset.yaml      # NEW FILE - Database deployment
│   │   ├── service.yaml          # NEW FILE - Database service
│   │   └── pvc.yaml              # NEW FILE - Persistent volume
│   ├── redis/
│   │   ├── deployment.yaml       # NEW FILE - Redis deployment
│   │   ├── service.yaml          # NEW FILE - Redis service
│   │   └── cluster.yaml          # NEW FILE - Redis cluster
│   └── ingress/
│       ├── nginx-ingress.yaml    # NEW FILE - Ingress controller
│       ├── ssl-certificates.yaml # NEW FILE - SSL configuration
│       └── external-dns.yaml     # NEW FILE - DNS automation
├── monitoring/
│   ├── prometheus/
│   │   ├── deployment.yaml       # NEW FILE - Prometheus deployment
│   │   ├── configmap.yaml        # NEW FILE - Prometheus config
│   │   └── rbac.yaml             # NEW FILE - Prometheus RBAC
│   ├── grafana/
│   │   ├── deployment.yaml       # NEW FILE - Grafana deployment
│   │   ├── configmap.yaml        # NEW FILE - Dashboard config
│   │   └── dashboards/           # NEW DIR - Dashboard definitions
│   └── alertmanager/
│       ├── deployment.yaml       # NEW FILE - AlertManager deployment
│       └── configmap.yaml        # NEW FILE - Alert configuration
└── istio/
    ├── gateway.yaml              # NEW FILE - Istio gateway
    ├── virtual-services.yaml     # NEW FILE - Traffic routing
    ├── destination-rules.yaml    # NEW FILE - Load balancing rules
    └── policies.yaml             # NEW FILE - Security policies
```

### Technical Implementation Details

**Kubernetes Deployment Configuration:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: planner-mcp-server
  namespace: production
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: planner-mcp-server
  template:
    metadata:
      labels:
        app: planner-mcp-server
        version: v1
    spec:
      containers:
      - name: mcp-server
        image: ghcr.io/intelligent-teams-planner/mcp-server:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: database-secrets\n              key: url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis-secrets\n              key: url\n```\n\n**Horizontal Pod Autoscaler Configuration:**\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: planner-mcp-server-hpa\n  namespace: production\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: planner-mcp-server\n  minReplicas: 3\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: requests_per_second\n      target:\n        type: AverageValue\n        averageValue: \"100\"\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 10\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 60\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n```\n\n**Istio Service Mesh Configuration:**\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: planner-gateway\n  namespace: production\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: planner-tls-secret\n    hosts:\n    - planner.example.com\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: planner-virtual-service\n  namespace: production\nspec:\n  hosts:\n  - planner.example.com\n  gateways:\n  - planner-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /api/\n    route:\n    - destination:\n        host: planner-mcp-server\n        port:\n          number: 8000\n      weight: 90\n    - destination:\n        host: planner-mcp-server-canary\n        port:\n          number: 8000\n      weight: 10\n    fault:\n      delay:\n        percentage:\n          value: 0.1\n        fixedDelay: 5s\n```\n\n### Environment Variables (Kubernetes Configuration)\n\n```env\n# Cluster Configuration\nKUBERNETES_VERSION=v1.28.0\nCNI_PLUGIN=calico\nETCD_BACKUP_ENABLED=true\nRBAC_ENABLED=true\nAUDIT_LOGGING_ENABLED=true\n\n# Auto-scaling Configuration\nHPA_ENABLED=true\nVPA_ENABLED=true\nCLUSTER_AUTOSCALER_ENABLED=true\nCUSTOM_METRICS_ENABLED=true\nPREDICTIVE_SCALING_ENABLED=true\n\n# Load Balancing\nINGRESS_CONTROLLER=nginx\nSSL_TERMINATION=ingress\nEXTERNAL_DNS_ENABLED=true\nRATE_LIMITING_ENABLED=true\n\n# Service Mesh\nSERVICE_MESH=istio\nMTLS_ENABLED=true\nTRAFFIC_ROUTING_ENABLED=true\nCIRCUIT_BREAKER_ENABLED=true\nDISTRIBUTED_TRACING_ENABLED=true\n\n# High Availability\nMULTI_ZONE_DEPLOYMENT=true\nPOD_DISRUPTION_BUDGET_ENABLED=true\nAUTO_FAILOVER_ENABLED=true\nPERSISTENT_VOLUME_REPLICATION=true\n\n# Resource Management\nNAMESPACE_QUOTAS_ENABLED=true\nQOS_CLASSES_ENABLED=true\nRESOURCE_MONITORING_ENABLED=true\nCOST_ALLOCATION_ENABLED=true\n\n# Deployment Strategy\nDEPLOYMENT_STRATEGY=rolling\nHEALTH_CHECKS_ENABLED=true\nAUTO_ROLLBACK_ENABLED=true\nCANARY_DEPLOYMENT_ENABLED=true\n\n# Monitoring\nPROMETHEUS_ENABLED=true\nGRAFANA_ENABLED=true\nAPM_ENABLED=true\nALERT_MANAGER_ENABLED=true\nSLI_SLO_MONITORING=true\n```\n\n### Resource Management Configuration\n\n**Namespace Resource Quotas:**\n```yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: production-quota\n  namespace: production\nspec:\n  hard:\n    requests.cpu: \"50\"\n    requests.memory: 100Gi\n    limits.cpu: \"100\"\n    limits.memory: 200Gi\n    persistentvolumeclaims: \"10\"\n    pods: \"50\"\n    secrets: \"20\"\n    services: \"20\"\n---\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: production-limits\n  namespace: production\nspec:\n  limits:\n  - default:\n      cpu: \"500m\"\n      memory: \"1Gi\"\n    defaultRequest:\n      cpu: \"100m\"\n      memory: \"256Mi\"\n    type: Container\n```\n\n**Pod Disruption Budget:**\n```yaml\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: planner-mcp-server-pdb\n  namespace: production\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: planner-mcp-server\n```\n\n### Monitoring and Observability\n\n**Prometheus Configuration:**\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\n  namespace: monitoring\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n    \n    rule_files:\n      - \"/etc/prometheus/rules/*.yml\"\n    \n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    \n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    \n    - job_name: 'kubernetes-nodes'\n      kubernetes_sd_configs:\n      - role: node\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n    \n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n```\n\n**Grafana Dashboards:**\n```json\n{\n  \"dashboard\": {\n    \"title\": \"Intelligent Teams Planner - Kubernetes Overview\",\n    \"panels\": [\n      {\n        \"title\": \"Cluster Resource Usage\",\n        \"type\": \"stat\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(container_cpu_usage_seconds_total[5m]))\",\n            \"legendFormat\": \"CPU Usage\"\n          },\n          {\n            \"expr\": \"sum(container_memory_usage_bytes)\",\n            \"legendFormat\": \"Memory Usage\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Pod Scaling Events\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"kube_deployment_status_replicas{deployment=~\\\".*planner.*\\\"}\",\n            \"legendFormat\": \"{{deployment}} replicas\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Request Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(istio_requests_total[5m])) by (destination_service_name)\",\n            \"legendFormat\": \"{{destination_service_name}}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Testing Strategy\n\n**Cluster Testing Framework:**\n- Kubernetes test suite for cluster validation\n- Chaos engineering with Chaos Monkey\n- Load testing with K6 and realistic traffic patterns\n- Disaster recovery testing with automated failover\n\n**Auto-scaling Validation:**\n```bash\n#!/bin/bash\n# Auto-scaling test script\n\n# Generate load to trigger HPA\nkubectl run load-generator --image=busybox --restart=Never -- /bin/sh -c \"while true; do wget -q -O- http://planner-mcp-server:8000/health; done\"\n\n# Monitor scaling events\nfor i in {1..30}; do\n  REPLICAS=$(kubectl get hpa planner-mcp-server-hpa -o jsonpath='{.status.currentReplicas}')\n  CPU_USAGE=$(kubectl get hpa planner-mcp-server-hpa -o jsonpath='{.status.currentCPUUtilizationPercentage}')\n  echo \"Time: ${i}min, Replicas: ${REPLICAS}, CPU: ${CPU_USAGE}%\"\n  sleep 60\ndone\n\n# Cleanup\nkubectl delete pod load-generator\n```\n\n**Service Mesh Testing:**\n```bash\n#!/bin/bash\n# Service mesh connectivity test\n\n# Test mutual TLS\nkubectl exec -it $(kubectl get pod -l app=planner-mcp-server -o jsonpath='{.items[0].metadata.name}') -- curl -k https://openwebui:8080/health\n\n# Test circuit breaker\nfor i in {1..100}; do\n  kubectl exec -it $(kubectl get pod -l app=planner-mcp-server -o jsonpath='{.items[0].metadata.name}') -- curl http://faulty-service:8080/\ndone\n\n# Verify traffic routing\nistioctl proxy-config routes $(kubectl get pod -l app=planner-mcp-server -o jsonpath='{.items[0].metadata.name}')\n```\n\n### Monitoring and Alerting\n\n**Key Metrics:**\n- Pod CPU and memory utilization\n- Horizontal and vertical scaling events\n- Service mesh request success rate and latency\n- Cluster node health and resource availability\n- Persistent volume usage and performance\n\n**Alert Rules:**\n```yaml\ngroups:\n- name: kubernetes-cluster\n  rules:\n  - alert: KubernetesNodeNotReady\n    expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Kubernetes node not ready\"\n      description: \"Node {{ $labels.node }} has been unready for more than 5 minutes\"\n  \n  - alert: PodCrashLooping\n    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Pod is crash looping\"\n      description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\"\n  \n  - alert: HPAScalingEvent\n    expr: increase(kube_hpa_status_current_replicas[5m]) > 2\n    for: 1m\n    labels:\n      severity: info\n    annotations:\n      summary: \"HPA scaling event detected\"\n      description: \"HPA {{ $labels.namespace }}/{{ $labels.hpa }} scaled by more than 2 replicas\"\n```\n\n## Change Log\n\n| Date | Version | Description | Author |\n|------|---------|-------------|--------|\n| 2025-10-06 | 1.0 | Initial container orchestration and scaling story | BMad Framework |\n\n## Dev Agent Record\n\n*This section will be populated by the development agent during implementation*\n\n### Agent Model Used\n*{{agent_model_name_version}}*\n\n### Debug Log References\n*Reference any debug logs or traces generated during development*\n\n### Completion Notes List\n*Notes about the completion of tasks and any issues encountered*\n\n### File List\n*List all files created, modified, or affected during story implementation*\n\n## QA Results\n\n*Results from QA Agent QA review of the completed story implementation*